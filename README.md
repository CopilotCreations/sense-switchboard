# Synesthesia Simulator

> Transform digital content into multi-sensory experiences

[![CI/CD Pipeline](https://github.com/your-username/synesthesia-simulator/actions/workflows/ci.yml/badge.svg)](https://github.com/your-username/synesthesia-simulator/actions/workflows/ci.yml)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## Overview

The Synesthesia Simulator is an interactive web application that converts website content into multi-sensory experiences. Just like synesthesiaâ€”a neurological phenomenon where stimulation of one sense triggers automatic experiences in anotherâ€”this application transforms:

- **Text â†’ Musical tones** using pentatonic scales
- **Colors â†’ Ambient sounds** and visual waveforms
- **Numbers â†’ Geometric patterns** and vibrations

## Features

âœ¨ **Real-time Content Transformation**
- Automatic content type detection
- Instant audio-visual feedback
- Smooth animations and transitions

ğŸµ **Audio Generation**
- Web Audio API synthesis
- Multiple waveform types
- Configurable volume and duration

ğŸ¨ **Visual Effects**
- Canvas-based particle systems
- Color-reactive animations
- Geometric pattern generation

âš™ï¸ **Customization**
- Adjustable mapping sliders
- User preference storage
- Preset management

## Quick Start

### Prerequisites

- Python 3.9 or higher
- Modern web browser with audio support

### Installation

```bash
# Clone the repository
git clone https://github.com/your-username/synesthesia-simulator.git
cd synesthesia-simulator

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Run the application
python run.py
```

Open http://127.0.0.1:5000 in your browser.

### Command Line Options

```bash
python run.py --host 0.0.0.0    # Allow external connections
python run.py --port 8080       # Use custom port
python run.py --debug           # Enable debug mode
```

## Usage

### Interactive Mode

1. Enter content in the input field:
   - **Text**: `Hello World`
   - **Color**: `#FF5733`
   - **Number**: `42`

2. Click "Experience" or press Enter

3. Adjust the sliders to customize:
   - Volume (0-100%)
   - Animation Speed (1-10x)
   - Visual Intensity (1-100%)

### API Mode

```python
import requests

# Detect content type
response = requests.post('http://localhost:5000/api/detect', 
    json={'content': '#FF0000'})
print(response.json())  # {'type': 'color', 'value': '#FF0000'}

# Map text to frequencies
response = requests.post('http://localhost:5000/api/map/text',
    json={'text': 'ABC', 'scale': 'pentatonic'})
print(response.json())
```

## Project Structure

```
sense-switchboard/
â”œâ”€â”€ run.py                    # Application entry point
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ .env.example              # Environment configuration
â”œâ”€â”€ .gitignore                # Git ignore patterns
â”œâ”€â”€ .github/workflows/        # CI/CD configuration
â”‚   â””â”€â”€ ci.yml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ frontend/             # Web interface
â”‚   â”‚   â”œâ”€â”€ index.html
â”‚   â”‚   â”œâ”€â”€ style.css
â”‚   â”‚   â””â”€â”€ main.js
â”‚   â””â”€â”€ backend/              # Flask server
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ server.py
â”œâ”€â”€ tests/                    # Test suite
â”‚   â”œâ”€â”€ test_content_mapper.py
â”‚   â”œâ”€â”€ test_user_preferences.py
â”‚   â””â”€â”€ test_api.py
â””â”€â”€ docs/                     # Documentation
    â”œâ”€â”€ ARCHITECTURE.md
    â”œâ”€â”€ USAGE.md
    â””â”€â”€ SUGGESTIONS.md
```

## Documentation

- [Architecture Guide](docs/ARCHITECTURE.md) - Technical design and system overview
- [User Guide](docs/USAGE.md) - Complete usage documentation
- [Suggestions](docs/SUGGESTIONS.md) - Future improvement ideas

## Testing

```bash
# Run all tests
pytest tests/ -v

# Run with coverage
pytest tests/ -v --cov=src --cov-report=html

# Run specific test file
pytest tests/test_content_mapper.py -v
```

## API Reference

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/health` | GET | Health check |
| `/api/detect` | POST | Detect content type |
| `/api/map/text` | POST | Map text to frequencies |
| `/api/map/color` | POST | Map color to sound |
| `/api/map/number` | POST | Map number to pattern |
| `/api/map/auto` | POST | Auto-detect and map |
| `/api/preferences` | GET/POST | User preferences |

## Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- Inspired by synesthesia research and multi-sensory experiences
- Built with Flask, Web Audio API, and Canvas
- Thanks to all contributors and testers
